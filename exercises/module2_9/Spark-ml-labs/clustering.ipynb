{"nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython2", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "codemirror_mode": {"name": "ipython", "version": 2}}, "kernelspec": {"language": "python", "name": "python2-spark20", "display_name": "Python 2 with Spark 2.0"}}, "cells": [{"execution_count": 1, "source": "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/kmeans_data.txt", "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2017-08-05 14:57:25--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/kmeans_data.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 72 [text/plain]\nSaving to: \u2018kmeans_data.txt\u2019\n\n100%[======================================>] 72          --.-K/s   in 0s      \n\n2017-08-05 14:57:26 (14.5 MB/s) - \u2018kmeans_data.txt\u2019 saved [72/72]\n\n"}], "cell_type": "code", "metadata": {}}, {"execution_count": 3, "source": "from numpy import array\nfrom math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans, KMeansModel\n\n# Load and parse the data\ndata = sc.textFile(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/kmeans_data.txt\")\nparsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n\n# Build the model (cluster the data)\nclusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusters.centers[clusters.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Within Set Sum of Squared Error = 0.692820323028\n"}], "cell_type": "code", "metadata": {}}, {"execution_count": 4, "source": "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/gmm_data.txt", "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2017-08-05 15:07:24--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/gmm_data.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.180.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.180.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 63973 (62K) [text/plain]\nSaving to: \u2018gmm_data.txt\u2019\n\n100%[======================================>] 63,973      --.-K/s   in 0.03s   \n\n2017-08-05 15:07:25 (1.98 MB/s) - \u2018gmm_data.txt\u2019 saved [63973/63973]\n\n"}], "cell_type": "code", "metadata": {}}, {"execution_count": 6, "source": "from numpy import array\n\nfrom pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n\n# Load and parse the data\ndata = sc.textFile(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/gmm_data.txt\")\nparsedData = data.map(lambda line: array([float(x) for x in line.strip().split(' ')]))\n\n# Build the model (cluster the data)\ngmm = GaussianMixture.train(parsedData, 2)\n\n\n\n# output parameters of model\nfor i in range(2):\n    print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n          \"sigma = \", gmm.gaussians[i].sigma.toArray())", "outputs": [{"name": "stdout", "output_type": "stream", "text": "('weight = ', 0.51938592841737807, 'mu = ', DenseVector([-0.1045, 0.0429]), 'sigma = ', array([[ 4.90752718, -2.00701532],\n       [-2.00701532,  1.01153216]]))\n('weight = ', 0.48061407158262198, 'mu = ', DenseVector([0.0722, 0.0167]), 'sigma = ', array([[ 4.77927512,  1.87599482],\n       [ 1.87599482,  0.91458438]]))\n"}], "cell_type": "code", "metadata": {}}, {"execution_count": null, "source": "", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}], "nbformat": 4}