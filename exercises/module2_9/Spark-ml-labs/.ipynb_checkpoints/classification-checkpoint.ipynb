{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.11\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.38198757764\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_svm_data.txt\")\n",
    "#print data.collect()\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(parsedData, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_svm_data.txt\")\n",
    "\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘sample_svm_data.txt’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r sample_svm_data.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-08-05 13:43:21--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.180.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.180.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104736 (102K) [text/plain]\n",
      "Saving to: ‘sample_libsvm_data.txt.1’\n",
      "\n",
      "100%[======================================>] 104,736     --.-K/s   in 0.02s   \n",
      "\n",
      "2017-08-05 13:43:22 (5.91 MB/s) - ‘sample_libsvm_data.txt.1’ saved [104736/104736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NAvie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy 0.975609756098\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_libsvm_data.txt.1\")\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Gradient Boost Tree Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.037037037037\n",
      "Learned classification GBT model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 406 <= 72.0)\n",
      "     If (feature 100 <= 165.0)\n",
      "      Predict: -1.0\n",
      "     Else (feature 100 > 165.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 406 > 72.0)\n",
      "     Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 433 <= 0.0)\n",
      "     If (feature 379 <= 251.0)\n",
      "      If (feature 263 <= 145.0)\n",
      "       Predict: -0.4768116880884702\n",
      "      Else (feature 263 > 145.0)\n",
      "       Predict: -0.4768116880884703\n",
      "     Else (feature 379 > 251.0)\n",
      "      Predict: 0.4768116880884694\n",
      "    Else (feature 433 > 0.0)\n",
      "     Predict: 0.4768116880884701\n",
      "  Tree 2:\n",
      "    If (feature 434 <= 0.0)\n",
      "     If (feature 627 <= 0.0)\n",
      "      Predict: 0.4381935810427206\n",
      "     Else (feature 627 > 0.0)\n",
      "      If (feature 209 <= 0.0)\n",
      "       Predict: -0.4381935810427206\n",
      "      Else (feature 209 > 0.0)\n",
      "       Predict: -0.4381935810427206\n",
      "    Else (feature 434 > 0.0)\n",
      "     If (feature 188 <= 252.0)\n",
      "      Predict: 0.4381935810427207\n",
      "     Else (feature 188 > 252.0)\n",
      "      Predict: 0.43819358104272155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_libsvm_data.txt.1\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#         (b) Use more iterations in practice.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=3)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification GBT model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0322580645161\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 2 with 5 nodes\n",
      "  If (feature 406 <= 20.0)\n",
      "   If (feature 100 <= 165.0)\n",
      "    Predict: 0.0\n",
      "   Else (feature 100 > 165.0)\n",
      "    Predict: 1.0\n",
      "  Else (feature 406 > 20.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_libsvm_data.txt.1\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0333333333333\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 512 <= 0.0)\n",
      "     If (feature 315 <= 144.0)\n",
      "      If (feature 486 <= 192.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 486 > 192.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 315 > 144.0)\n",
      "      Predict: 0.0\n",
      "    Else (feature 512 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 351 <= 2.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 351 > 2.0)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 301 <= 0.0)\n",
      "     If (feature 291 <= 102.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 291 > 102.0)\n",
      "      Predict: 0.0\n",
      "    Else (feature 301 > 0.0)\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb3f-2e8796b7817263-c965b24fe08f/notebook/work/sample_libsvm_data.txt.1\")\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
